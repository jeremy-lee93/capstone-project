{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll take the same approach to modeling for tweets as I did with the headlines. I'll be using both the Multinomial Naive Bayes and Random Forests algorithm to build and evaluate model performance for both 4 classes and 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:31.805894Z",
     "start_time": "2021-06-07T21:44:27.553087Z"
    }
   },
   "outputs": [],
   "source": [
    "#import libraries \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:31.833139Z",
     "start_time": "2021-06-07T21:44:31.808515Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:31.852118Z",
     "start_time": "2021-06-07T21:44:31.843070Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:32.279231Z",
     "start_time": "2021-06-07T21:44:31.856054Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter_df = pd.read_csv('data/nba_twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:32.298796Z",
     "start_time": "2021-06-07T21:44:32.281601Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>source</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>dual_labels</th>\n",
       "      <th>trip_labels</th>\n",
       "      <th>quad_labels</th>\n",
       "      <th>five_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-26 12:35:27</td>\n",
       "      <td>Obi Toppin is expected to participate in the N...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>['obi', 'toppin', 'expected', 'participate', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2021-02-25 23:08:17</td>\n",
       "      <td>MPJ SLAM</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>['mpj', 'slam', '']</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-25 17:48:50</td>\n",
       "      <td>Timberwolves G Malik Beasley has been suspende...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>['timberwolves', 'g', 'malik', 'beasley', 'sus...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2021-02-25 13:40:20</td>\n",
       "      <td>Luka's game-winner against the Celtics vs. Luk...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>['lukas', 'gamewinner', 'celtic', 'v', 'lukas'...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2021-02-25 13:10:50</td>\n",
       "      <td>It's time to stop underrating the Utah Jazz Fr...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>['time', 'stop', 'underrating', 'utah', 'jazz'...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              tweet  \\\n",
       "0  2021-02-26 12:35:27  Obi Toppin is expected to participate in the N...   \n",
       "1  2021-02-25 23:08:17                                          MPJ SLAM    \n",
       "2  2021-02-25 17:48:50  Timberwolves G Malik Beasley has been suspende...   \n",
       "3  2021-02-25 13:40:20  Luka's game-winner against the Celtics vs. Luk...   \n",
       "4  2021-02-25 13:10:50  It's time to stop underrating the Utah Jazz Fr...   \n",
       "\n",
       "  source                                       tweet_tokens  dual_labels  \\\n",
       "0  Yahoo  ['obi', 'toppin', 'expected', 'participate', '...            0   \n",
       "1  Yahoo                                ['mpj', 'slam', '']            1   \n",
       "2  Yahoo  ['timberwolves', 'g', 'malik', 'beasley', 'sus...            1   \n",
       "3  Yahoo  ['lukas', 'gamewinner', 'celtic', 'v', 'lukas'...            1   \n",
       "4  Yahoo  ['time', 'stop', 'underrating', 'utah', 'jazz'...            1   \n",
       "\n",
       "   trip_labels  quad_labels  five_labels  \n",
       "0            0            3            1  \n",
       "1            2            1            2  \n",
       "2            2            1            2  \n",
       "3            2            2            2  \n",
       "4            2            1            2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:32.327104Z",
     "start_time": "2021-06-07T21:44:32.301518Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter_df.dropna(subset=['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:32.337673Z",
     "start_time": "2021-06-07T21:44:32.329530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    101191\n",
       "0     25840\n",
       "Name: dual_labels, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['dual_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:32.356156Z",
     "start_time": "2021-06-07T21:44:32.341650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    94828\n",
       "0    24695\n",
       "1     7508\n",
       "Name: trip_labels, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['trip_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:32.371390Z",
     "start_time": "2021-06-07T21:44:32.360049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    93218\n",
       "3    24168\n",
       "2     6822\n",
       "0     2823\n",
       "Name: quad_labels, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['quad_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:32.382831Z",
     "start_time": "2021-06-07T21:44:32.373965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    96911\n",
       "1    20803\n",
       "4     3978\n",
       "3     2715\n",
       "0     2624\n",
       "Name: five_labels, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['five_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:39.796282Z",
     "start_time": "2021-06-07T21:44:39.791793Z"
    }
   },
   "outputs": [],
   "source": [
    "def summary_scores(model, train_set_x, test_set_x, train_set_y, test_set_y, cross_val_var):\n",
    "    print(f'Training Accuracy: {model.score(train_set_x, train_set_y)}')\n",
    "    print(f'Cross Validation Accuracy: {np.mean(cross_val_var)}')\n",
    "    print(f'Testing Accuracy: {model.score(test_set_x, test_set_y)}')\n",
    "    print(f'F1 Score: {f1_score(test_set_y, model.predict(test_set_x), average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:43.647714Z",
     "start_time": "2021-06-07T21:44:43.562227Z"
    }
   },
   "outputs": [],
   "source": [
    "X = twitter_df['tweet']\n",
    "y_four = twitter_df['quad_labels']\n",
    "y_five = twitter_df['five_labels']\n",
    "X_train_four, X_test_four, y_train_four, y_test_four = train_test_split(X, y_four, \n",
    "                                                                        test_size=0.3, \n",
    "                                                                        stratify=y_four, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:44.783732Z",
     "start_time": "2021-06-07T21:44:44.687162Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_five, X_test_five, y_train_five, y_test_five = train_test_split(X, y_five, \n",
    "                                                                        test_size=0.3, \n",
    "                                                                        stratify=y_five, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:46.635111Z",
     "start_time": "2021-06-07T21:44:45.056743Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_four = TfidfVectorizer(stop_words=stopwords)\n",
    "X_train_sparse_four = vectorizer_four.fit_transform(X_train_four)\n",
    "X_test_sparse_four = vectorizer_four.transform(X_test_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:48.298115Z",
     "start_time": "2021-06-07T21:44:46.637249Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_five = TfidfVectorizer(stop_words=stopwords)\n",
    "X_train_sparse_five = vectorizer_five.fit_transform(X_train_five)\n",
    "X_test_sparse_five = vectorizer_five.transform(X_test_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, when using 4 classes the topics associated with each group are:\n",
    "\n",
    "0. Free Agency\n",
    "1. LeBron, the Warriors, coaches, and highlights\n",
    "2. Game Scores/Outcomes\n",
    "3. NBA Draft/Rankings\n",
    "\n",
    "For 5 classes the topics are:\n",
    "\n",
    "0. Rankings\n",
    "1. League News (keyword: 'NBA')\n",
    "2. Tweets focused on individual players and their performances\n",
    "3. Free Agency\n",
    "4. NBA Draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the multinomial naive bayes algorithm we cannot do a PCA transformation first because the algorithm requires that all the values be positive (probabilities can only exist between the values of zero and one). We will still be using the sparse matrix as the input so the algorithm should work efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:48.344225Z",
     "start_time": "2021-06-07T21:44:48.300497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_vanilla_four = MultinomialNB()\n",
    "mnb_vanilla_four.fit(X_train_sparse_four, y_train_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:51.913090Z",
     "start_time": "2021-06-07T21:44:48.501497Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb_vanilla_cv = cross_val_score(mnb_vanilla_four, X_train_sparse_four, \n",
    "                                 y_train_four, cv=5, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:44:51.997947Z",
     "start_time": "2021-06-07T21:44:51.917981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8378448285556842\n",
      "Cross Validation Accuracy: 0.8199975427492703\n",
      "Testing Accuracy: 0.8243768039884545\n",
      "F1 Score: 0.7864414022814671\n"
     ]
    }
   ],
   "source": [
    "summary_scores(mnb_vanilla_four, X_train_sparse_four, \n",
    "               X_test_sparse_four, y_train_four, y_test_four, mnb_vanilla_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T21:45:05.133469Z",
     "start_time": "2021-06-07T21:45:05.053384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.19      0.32       847\n",
      "           1       0.82      0.99      0.89     27966\n",
      "           2       1.00      0.04      0.07      2047\n",
      "           3       0.88      0.49      0.63      7250\n",
      "\n",
      "    accuracy                           0.82     38110\n",
      "   macro avg       0.92      0.43      0.48     38110\n",
      "weighted avg       0.84      0.82      0.79     38110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_four, mnb_vanilla_four.predict(X_test_sparse_four)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T03:09:33.495054Z",
     "start_time": "2021-03-23T03:09:33.288693Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb_vanilla_five = MultinomialNB()\n",
    "mnb_vanilla_five.fit(X_train_sparse_five, y_train_five)\n",
    "mnb_vanilla_cv_five = cross_val_score(mnb_vanilla_five, \n",
    "                                      X_train_sparse_five, y_train_five, cv=5, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T03:09:33.523758Z",
     "start_time": "2021-03-23T03:09:33.497266Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_scores(mnb_vanilla_five, X_train_sparse_five, \n",
    "               X_test_sparse_five, y_train_five, y_test_five, mnb_vanilla_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T01:22:44.114947Z",
     "start_time": "2021-03-26T01:22:43.811329Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test_five, mnb_vanilla_five.predict(X_test_sparse_five)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have a larger dataset, the naive bayes algorithm do slightly worse (when compared to the classification of headlines) for predicting the classes of tweets. There is a greater than 1% difference for the model trained with 5 classes so there may be some overfitting occurring. We would either need to change the size of the training and test sets or try to gather more data for the classes.\n",
    "\n",
    "In the context of using a classification model as part of a recommendation system, the low recall scores are somewhat disappointing. In this context of a recommendation system precision is probably more important than recall but we would certainly like to have recall scores above 14% (for the 0 class) and 21% (for the 3 class). The low recall means that we have a lot of false negatives which means that users of a recommendation system might be missing out on content that they enjoy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T03:11:12.394387Z",
     "start_time": "2021-03-23T03:09:33.525877Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_rfc_vanilla_four = Pipeline([('pca', TruncatedSVD(n_components=100, random_state=23)),\n",
    "                    ('rfc', RandomForestClassifier())])\n",
    "\n",
    "pipe_rfc_vanilla_four.fit(X_train_sparse_four, y_train_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T03:15:23.219077Z",
     "start_time": "2021-03-23T03:11:12.396703Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_rfc_vanilla_cv_four = cross_val_score(pipe_rfc_vanilla_four, \n",
    "                                           X_train_sparse_four, y_train_four, \n",
    "                                           cv=5, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T03:15:24.999820Z",
     "start_time": "2021-03-23T03:15:23.221123Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_scores(pipe_rfc_vanilla_four, X_train_sparse_four, X_test_sparse_four, y_train_four, \n",
    "               y_test_four, pipe_rfc_vanilla_cv_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T03:15:25.872250Z",
     "start_time": "2021-03-23T03:15:25.002286Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plt.grid(False)\n",
    "plot_confusion_matrix(pipe_rfc_vanilla_four, X_test_sparse_four, y_test_four, cmap='Blues', ax=ax)\n",
    "plt.savefig('tweets_random_forest_conf_matrix_4_classes.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T03:15:26.496258Z",
     "start_time": "2021-03-23T03:15:25.874611Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test_four, pipe_rfc_vanilla_four.predict(X_test_sparse_four)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T03:17:07.877441Z",
     "start_time": "2021-03-23T03:15:26.498497Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_rfc_vanilla_five = Pipeline([('pca', TruncatedSVD(n_components=100, random_state=23)),\n",
    "                                 ('rfc', RandomForestClassifier())])\n",
    "pipe_rfc_vanilla_five.fit(X_train_sparse_five, y_train_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T03:56:40.356606Z",
     "start_time": "2021-03-23T03:52:29.251850Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_rfc_vanilla_cv_five = cross_val_score(pipe_rfc_vanilla_five, X_train_sparse_five, y_train_five,\n",
    "                                          cv=5, n_jobs=-1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:07:43.310030Z",
     "start_time": "2021-03-23T17:07:41.547057Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_scores(pipe_rfc_vanilla_five, X_train_sparse_five, X_test_sparse_five, y_train_five, \n",
    "               y_test_five, pipe_rfc_vanilla_cv_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:07:46.828453Z",
     "start_time": "2021-03-23T17:07:46.031820Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plt.grid(False)\n",
    "plot_confusion_matrix(pipe_rfc_vanilla_five, X_test_sparse_five, y_test_five, cmap='Blues', ax=ax)\n",
    "plt.savefig('tweets_random_forest_conf_matrix_5_classes.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:07:48.375426Z",
     "start_time": "2021-03-23T17:07:47.794376Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test_five, pipe_rfc_vanilla_five.predict(X_test_sparse_five)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest models for both 4 and 5 classes perform much better across for accuracy, precison, and recall. The testing accuracy is nearly identical to the cross validation training accuracy so we don't suffer from overfitting.  In the context of a recommendation system, the high precision and recall scores would mean that users are going to get the exact type of content that they want to see. This is important because it would help to prevent something like customer churn. Given that the accuracy is very high for this model, further testing would be necssary using new data that wasn't used as part of the clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As with headlines, the random forest algorithm seems to be the better of the two algorithms in terms of classifying the topics and is recommended if a model were to be put into production.\n",
    "\n",
    "It's best to take a top down approach when identifying topics within NBA discourse. Model performance does suffer slightly when there are a greater number of topics (which is expected), so it is better to start broad and become more granular only as necessary. More data has to be collected as we become more granular because more specificity leads to fewer examples per class.\n",
    "\n",
    "Free agency and the NBA Draft are two major topics that were found using both four and 5 clusters. Although the offseason won't garner the same attention as the regular season or playoffs, the NBA and its media partners should think of ways of how to market the draft and free agency to the casual fan. There were more tweets related to the draft/free agency than rankings, so we might look to promote these events more to increase engagement over the entire year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
